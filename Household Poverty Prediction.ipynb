{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Household Poverty Prediction: Application of Featuretools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this recipe, we will try to perdict the Poverty level of Households in Costa Rica through Automated Feature Engineering using Featuretools. The data has been take from a Kaggle Competition. For more details of the competition, please refer the below link:<br>\n",
    "https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data <br>\n",
    "\n",
    "To know more about Featuretools, please refer the below link: <br>\n",
    "https://docs.featuretools.com/en/stable/#minute-quick-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective of the recipe\n",
    "This recipe will try to explain how one can create multiple features with minimal effort and can achieve decent result vis-a-vis the top score in the competition.<br>\n",
    "The application of feature tools for feature creation can be used as a starting point for any model building exercise. This recipe will try to enable the user how it can be done in few steps. Please note that this recipe is the simplistic version of application of feature tools, for more advance ones, please refer to other recipes on Automated Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the relevant libraries required for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import featuretools as ft\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e511a5ea8fff2f8ce3ac3a457eca2feeaacbb037"
   },
   "source": [
    "- We'll read in the data and join the training and testing set together.\n",
    "- The train data will be used for training the model and test data will be used for prediction and submission purpose\n",
    "- Since the test data doesn't have 'Target'column, we are creating a dummy one with all missing which will be used for appending both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9557 rows and 142  feature columns(excluding TARGET column) in train dataframe\n",
      "There are 23856 rows and 142 feature columns in test dataframe\n",
      "There are 33413 rows and 142 feature columns(excluding TARGET column) in appended dataframe\n"
     ]
    }
   ],
   "source": [
    "# Raw data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(f'There are {train.shape[0]} rows and {train.shape[1]-1}  feature columns(excluding TARGET column) in train dataframe')\n",
    "print(f'There are {test.shape[0]} rows and {test.shape[1]} feature columns in test dataframe')\n",
    "\n",
    "test['Target'] = np.nan\n",
    "\n",
    "data = train.append(test, sort = True)\n",
    "print(f'There are {data.shape[0]} rows and {data.shape[1]-1} feature columns(excluding TARGET column) in appended dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SQBage</th>\n",
       "      <th>SQBdependency</th>\n",
       "      <th>SQBedjefe</th>\n",
       "      <th>SQBescolari</th>\n",
       "      <th>SQBhogar_nin</th>\n",
       "      <th>SQBhogar_total</th>\n",
       "      <th>SQBmeaned</th>\n",
       "      <th>SQBovercrowding</th>\n",
       "      <th>Target</th>\n",
       "      <th>...</th>\n",
       "      <th>television</th>\n",
       "      <th>tipovivi1</th>\n",
       "      <th>tipovivi2</th>\n",
       "      <th>tipovivi3</th>\n",
       "      <th>tipovivi4</th>\n",
       "      <th>tipovivi5</th>\n",
       "      <th>v14a</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>v2a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_279628684</td>\n",
       "      <td>1849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_f29eb3ddd</td>\n",
       "      <td>4489</td>\n",
       "      <td>64.0</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_68de51c94</td>\n",
       "      <td>8464</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>121.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_d671db89c</td>\n",
       "      <td>289</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121</td>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_d56d6f5f5</td>\n",
       "      <td>1369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id  SQBage  SQBdependency  SQBedjefe  SQBescolari  SQBhogar_nin  \\\n",
       "0  ID_279628684    1849            0.0        100          100             0   \n",
       "1  ID_f29eb3ddd    4489           64.0        144          144             0   \n",
       "2  ID_68de51c94    8464           64.0          0          121             0   \n",
       "3  ID_d671db89c     289            1.0        121           81             4   \n",
       "4  ID_d56d6f5f5    1369            1.0        121          121             4   \n",
       "\n",
       "   SQBhogar_total  SQBmeaned  SQBovercrowding  Target  ...  television  \\\n",
       "0               1      100.0         1.000000     4.0  ...           0   \n",
       "1               1      144.0         1.000000     4.0  ...           0   \n",
       "2               1      121.0         0.250000     4.0  ...           0   \n",
       "3              16      121.0         1.777778     4.0  ...           0   \n",
       "4              16      121.0         1.777778     4.0  ...           0   \n",
       "\n",
       "   tipovivi1  tipovivi2  tipovivi3  tipovivi4  tipovivi5  v14a  v18q  v18q1  \\\n",
       "0          0          0          1          0          0     1     0    NaN   \n",
       "1          0          0          1          0          0     1     1    1.0   \n",
       "2          1          0          0          0          0     1     0    NaN   \n",
       "3          0          0          1          0          0     1     1    1.0   \n",
       "4          0          0          1          0          0     1     1    1.0   \n",
       "\n",
       "       v2a1  \n",
       "0  190000.0  \n",
       "1  135000.0  \n",
       "2       NaN  \n",
       "3  180000.0  \n",
       "4  180000.0  \n",
       "\n",
       "[5 rows x 143 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SQBage</th>\n",
       "      <th>SQBdependency</th>\n",
       "      <th>SQBedjefe</th>\n",
       "      <th>SQBescolari</th>\n",
       "      <th>SQBhogar_nin</th>\n",
       "      <th>SQBhogar_total</th>\n",
       "      <th>SQBmeaned</th>\n",
       "      <th>SQBovercrowding</th>\n",
       "      <th>Target</th>\n",
       "      <th>...</th>\n",
       "      <th>television</th>\n",
       "      <th>tipovivi1</th>\n",
       "      <th>tipovivi2</th>\n",
       "      <th>tipovivi3</th>\n",
       "      <th>tipovivi4</th>\n",
       "      <th>tipovivi5</th>\n",
       "      <th>v14a</th>\n",
       "      <th>v18q</th>\n",
       "      <th>v18q1</th>\n",
       "      <th>v2a1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23851</th>\n",
       "      <td>ID_a065a7cad</td>\n",
       "      <td>100</td>\n",
       "      <td>0.25</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>33.0625</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23852</th>\n",
       "      <td>ID_1a7c6953b</td>\n",
       "      <td>2916</td>\n",
       "      <td>1.00</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23853</th>\n",
       "      <td>ID_07dbb4be2</td>\n",
       "      <td>144</td>\n",
       "      <td>1.00</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23854</th>\n",
       "      <td>ID_34d2ed046</td>\n",
       "      <td>144</td>\n",
       "      <td>1.00</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23855</th>\n",
       "      <td>ID_34754556f</td>\n",
       "      <td>2601</td>\n",
       "      <td>1.00</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>36.0000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id  SQBage  SQBdependency  SQBedjefe  SQBescolari  \\\n",
       "23851  ID_a065a7cad     100           0.25         25            9   \n",
       "23852  ID_1a7c6953b    2916           1.00         36           36   \n",
       "23853  ID_07dbb4be2     144           1.00         36           16   \n",
       "23854  ID_34d2ed046     144           1.00         36           25   \n",
       "23855  ID_34754556f    2601           1.00         36           36   \n",
       "\n",
       "       SQBhogar_nin  SQBhogar_total  SQBmeaned  SQBovercrowding  Target  ...  \\\n",
       "23851             4              36    33.0625             36.0     NaN  ...   \n",
       "23852             4              16    36.0000              4.0     NaN  ...   \n",
       "23853             4              16    36.0000              4.0     NaN  ...   \n",
       "23854             4              16    36.0000              4.0     NaN  ...   \n",
       "23855             4              16    36.0000              4.0     NaN  ...   \n",
       "\n",
       "       television  tipovivi1  tipovivi2  tipovivi3  tipovivi4  tipovivi5  \\\n",
       "23851           0          1          0          0          0          0   \n",
       "23852           0          1          0          0          0          0   \n",
       "23853           0          1          0          0          0          0   \n",
       "23854           0          1          0          0          0          0   \n",
       "23855           0          1          0          0          0          0   \n",
       "\n",
       "       v14a  v18q  v18q1  v2a1  \n",
       "23851     1     0    NaN   NaN  \n",
       "23852     1     0    NaN   NaN  \n",
       "23853     1     0    NaN   NaN  \n",
       "23854     1     0    NaN   NaN  \n",
       "23855     1     0    NaN   NaN  \n",
       "\n",
       "[5 rows x 143 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give a look at the glimpse of the data:\n",
    "- The column names are given in abbreviation form, to understand more details of each column, please refer the link of kaggle competetion mentioned above in the beginning of the recipe<br>\n",
    "<br>\n",
    "- However to understand the recipe, let's focus on the core data fields:\n",
    "    - **Id** - a unique identifier for each row.\n",
    "    - **Target** - the target is an ordinal variable indicating groups of income levels.\n",
    "        - 1 = extreme poverty\n",
    "        - 2 = moderate poverty\n",
    "        - 3 = vulnerable households\n",
    "        - 4 = non vulnerable households\n",
    "    - **idhogar** - Household ID\n",
    "    - **parentesco1** - indicates if this person is the head of the household."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example household. Please note that the anlaysis will done at household level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idhogar</th>\n",
       "      <th>parentesco1</th>\n",
       "      <th>Id</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4433</th>\n",
       "      <td>493f97dcb</td>\n",
       "      <td>0</td>\n",
       "      <td>ID_d9c6b628b</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>493f97dcb</td>\n",
       "      <td>0</td>\n",
       "      <td>ID_6d4c95642</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4435</th>\n",
       "      <td>493f97dcb</td>\n",
       "      <td>1</td>\n",
       "      <td>ID_d45817f89</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4436</th>\n",
       "      <td>493f97dcb</td>\n",
       "      <td>0</td>\n",
       "      <td>ID_e4d7b971a</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>493f97dcb</td>\n",
       "      <td>0</td>\n",
       "      <td>ID_ab933e7ab</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438</th>\n",
       "      <td>493f97dcb</td>\n",
       "      <td>0</td>\n",
       "      <td>ID_009603cd8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>493f97dcb</td>\n",
       "      <td>0</td>\n",
       "      <td>ID_7d548c06c</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>493f97dcb</td>\n",
       "      <td>0</td>\n",
       "      <td>ID_878a0bf87</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        idhogar  parentesco1            Id  Target\n",
       "4433  493f97dcb            0  ID_d9c6b628b       4\n",
       "4434  493f97dcb            0  ID_6d4c95642       4\n",
       "4435  493f97dcb            1  ID_d45817f89       4\n",
       "4436  493f97dcb            0  ID_e4d7b971a       4\n",
       "4437  493f97dcb            0  ID_ab933e7ab       4\n",
       "4438  493f97dcb            0  ID_009603cd8       4\n",
       "4439  493f97dcb            0  ID_7d548c06c       4\n",
       "4440  493f97dcb            0  ID_878a0bf87       4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train['idhogar']=='493f97dcb',['idhogar','parentesco1','Id', 'Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "fe3939df5b92af0efa9f3d3c61e4a7b393b4d0f5"
   },
   "outputs": [],
   "source": [
    "train_valid = train.loc[train['parentesco1'] == 1, ['idhogar', 'Id', 'Target']].copy()\n",
    "test_valid = test.loc[test['parentesco1'] == 1, ['idhogar', 'Id']].copy()\n",
    "\n",
    "submission_base = test[['Id', 'idhogar']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "\n",
    "  \n",
    "    mapping = {\"yes\": 1, \"no\": 0}\n",
    "\n",
    "    # Fill in the values with the correct mapping\n",
    "    data['dependency'] = data['dependency'].replace(mapping).astype(np.float64)\n",
    "    data['edjefa'] = data['edjefa'].replace(mapping).astype(np.float64)\n",
    "    data['edjefe'] = data['edjefe'].replace(mapping).astype(np.float64)\n",
    "\n",
    "\n",
    "    \n",
    "    ## Missing Values\n",
    "    data['v18q1'] = data['v18q1'].fillna(0)\n",
    "\n",
    "    # Fill in households that own the house with 0 rent payment\n",
    "    data.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n",
    "\n",
    "    # Create missing rent payment column\n",
    "    data['v2a1-missing'] = data['v2a1'].isnull()\n",
    "\n",
    "    # If individual is over 19 or younger than 7 and missing years behind, set it to 0\n",
    "    data.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n",
    "\n",
    "    # Add a flag for those between 7 and 19 with a missing value\n",
    "    data['rez_esc-missing'] = data['rez_esc'].isnull()\n",
    "\n",
    "    data.loc[data['rez_esc'] > 5, 'rez_esc'] = 5\n",
    "    \n",
    "    \n",
    "    ## Domain Knowledge Feature Construction\n",
    "    # Difference between people living in house and household size\n",
    "    data['hhsize-diff'] = data['tamviv'] - data['hhsize']\n",
    "\n",
    "    elec = []\n",
    "\n",
    "    # Assign values\n",
    "    for i, row in data.iterrows():\n",
    "        if row['noelec'] == 1:\n",
    "            elec.append(0)\n",
    "        elif row['coopele'] == 1:\n",
    "            elec.append(1)\n",
    "        elif row['public'] == 1:\n",
    "            elec.append(2)\n",
    "        elif row['planpri'] == 1:\n",
    "            elec.append(3)\n",
    "        else:\n",
    "            elec.append(np.nan)\n",
    "\n",
    "    # Record the new variable and missing flag\n",
    "    data['elec'] = elec\n",
    "    data['elec-missing'] = data['elec'].isnull()\n",
    "\n",
    "    # Remove the electricity columns\n",
    "    # data = data.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])\n",
    "\n",
    "    # Wall ordinal variable\n",
    "    data['walls'] = np.argmax(np.array(data[['epared1', 'epared2', 'epared3']]),\n",
    "                               axis = 1)\n",
    "\n",
    "    # data = data.drop(columns = ['epared1', 'epared2', 'epared3'])\n",
    "\n",
    "    # Roof ordinal variable\n",
    "    data['roof'] = np.argmax(np.array(data[['etecho1', 'etecho2', 'etecho3']]),\n",
    "                               axis = 1)\n",
    "    # data = data.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n",
    "\n",
    "    # Floor ordinal variable\n",
    "    data['floor'] = np.argmax(np.array(data[['eviv1', 'eviv2', 'eviv3']]),\n",
    "                               axis = 1)\n",
    "    # data = data.drop(columns = ['eviv1', 'eviv2', 'eviv3'])\n",
    "\n",
    "    # Create new feature\n",
    "    data['walls+roof+floor'] = data['walls'] + data['roof'] + data['floor']\n",
    "\n",
    "    # No toilet, no electricity, no floor, no water service, no ceiling\n",
    "    data['warning'] = 1 * (data['sanitario1'] + \n",
    "                             (data['elec'] == 0) + \n",
    "                             data['pisonotiene'] + \n",
    "                             data['abastaguano'] + \n",
    "                             (data['cielorazo'] == 0))\n",
    "\n",
    "    # Owns a refrigerator, computer, tablet, and television\n",
    "    data['bonus'] = 1 * (data['refrig'] + \n",
    "                          data['computer'] + \n",
    "                          (data['v18q1'] > 0) + \n",
    "                          data['television'])\n",
    "\n",
    "    # Per capita features\n",
    "    data['phones-per-capita'] = data['qmobilephone'] / data['tamviv']\n",
    "    data['tablets-per-capita'] = data['v18q1'] / data['tamviv']\n",
    "    data['rooms-per-capita'] = data['rooms'] / data['tamviv']\n",
    "    data['rent-per-capita'] = data['v2a1'] / data['tamviv']\n",
    "\n",
    "    # Create one feature from the `instlevel` columns\n",
    "    data['inst'] = np.argmax(np.array(data[[c for c in data if c.startswith('instl')]]), axis = 1)\n",
    "    # data = data.drop(columns = [c for c in data if c.startswith('instlevel')])\n",
    "\n",
    "    data['escolari/age'] = data['escolari'] / data['age']\n",
    "    data['inst/age'] = data['inst'] / data['age']\n",
    "    data['tech'] = data['v18q'] + data['mobilephone']\n",
    "\n",
    "    ## Remove Squared Variables\n",
    "    # The gradient boosting machine does not need the squared version of variables it if already has the original variables. \n",
    "\n",
    "    data = data[[x for x in data if not x.startswith('SQB')]]\n",
    "    data = data.drop(columns = ['agesq'])\n",
    "\n",
    "    ## Remove Highly Correlated Columns\n",
    "\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.975)]\n",
    "\n",
    "    print(f'There are {len(to_drop)} correlated columns to remove.')\n",
    "    print(to_drop)\n",
    "\n",
    "    data = data.drop(columns = to_drop)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 correlated columns to remove.\n",
      "['area2', 'hogar_total', 'male', 'public', 'r4t3', 'tamhog', 'elec']\n"
     ]
    }
   ],
   "source": [
    "data = preprocess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7bfb23571103d16934da45554ba02d4f915e9519"
   },
   "source": [
    "#  Establish Correct Variable Types\n",
    "\n",
    "We need to specify the correct variables types:\n",
    "\n",
    "1. Individual Variables: these are characteristics of each individual rather than the household\n",
    "    * Boolean: Yes or No (0 or 1)\n",
    "    * Ordered Discrete: Integers with an ordering\n",
    "2. Household variables\n",
    "    * Boolean: Yes or No\n",
    "    * Ordered Discrete: Integers with an ordering\n",
    "    * Continuous numeric\n",
    "\n",
    "Below we manually define the variables in each category. This is a little tedious, but also necessary. There are some more opeations being done in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert rez_esc because of missing values.\n"
     ]
    }
   ],
   "source": [
    "hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n",
    "           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n",
    "           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n",
    "           'pisonatur', 'pisonotiene', 'pisomadera',\n",
    "           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n",
    "           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n",
    "            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n",
    "           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n",
    "           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n",
    "           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n",
    "           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n",
    "           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n",
    "           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n",
    "           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n",
    "           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing', 'elec-missing']\n",
    "\n",
    "hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n",
    "              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin','hhsize-diff',\n",
    "              'elec',  'walls', 'roof', 'floor', 'walls+roof+floor', 'warning', 'bonus',\n",
    "              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n",
    "\n",
    "hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding',\n",
    "          'phones-per-capita', 'tablets-per-capita', 'rooms-per-capita', 'rent-per-capita']\n",
    "\n",
    "\n",
    "ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n",
    "            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n",
    "            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n",
    "            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n",
    "            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n",
    "            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n",
    "            'instlevel9', 'mobilephone', 'rez_esc-missing']\n",
    "\n",
    "ind_ordered = ['age', 'escolari', 'rez_esc', 'inst', 'tech']\n",
    "\n",
    "ind_cont = ['escolari/age', 'inst/age']\n",
    "\n",
    "to_remove = []\n",
    "for l in [hh_ordered, hh_bool, hh_cont, ind_bool, ind_ordered, ind_cont]:\n",
    "    for c in l:\n",
    "        if c not in data:\n",
    "            to_remove.append(c)\n",
    "\n",
    "for l in [hh_ordered, hh_bool, hh_cont, ind_bool, ind_ordered, ind_cont]:\n",
    "    for c in to_remove:\n",
    "        if c in l:\n",
    "            l.remove(c)\n",
    "\n",
    "for variable in (hh_bool + ind_bool):\n",
    "    data[variable] = data[variable].astype('bool')\\\n",
    "\n",
    "for variable in (hh_cont + ind_cont):\n",
    "    data[variable] = data[variable].astype(float)\n",
    "\n",
    "for variable in (hh_ordered + ind_ordered):\n",
    "    try:\n",
    "        data[variable] = data[variable].astype(int)\n",
    "    except Exception as e:\n",
    "        print(f'Could not convert {variable} because of missing values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featuretools\n",
    "Featuretools is an open source library for performing automated feature engineering. It is a great tool designed to fast-forward the feature generation process, thereby giving more time to focus on other aspects of machine learning model building.<br> There are 3 components in feature tools that we need to focus upon for creation of automated features.\n",
    "\n",
    "- **Entity Set**: An Entity can be considered as a representation of a Pandas DataFrame. A collection of multiple entities is called an Entityset.<br/>\n",
    "<br/>\n",
    "- **Deep Feature Synthesis**: Deep Feature Synthesis (DFS) has got nothing to do with deep learning. Don’t worry. DFS is actually a Feature Engineering method and is the backbone of Featuretools. It enables the creation of new features from single, as well as multiple dataframes.<br/>\n",
    "<br/>\n",
    "- **Feature Primitives**: DFS create features by applying Feature primitives to the Entity-relationships in an EntitySet. These primitives are the often-used methods to generate features manually. For example, the primitive “mean” would find the mean of a variable at an aggregated level.\n",
    "\n",
    "Let's start using Feature Tools for automated feature engineering.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c94d438dddaa1c2e08259a3a2ed01118173d328f"
   },
   "source": [
    "# EntitySet and Entities\n",
    "\n",
    "An `EntitySet` in Featuretools holds all of the tables and the relationships between them. At the moment we only have a single table, but we can create multiple tables through normalization. We'll call the first table `data` since it contains all the information both at the individual level and at the household level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "63901e5cabcba3760dd711f9086d4023dca4eb7e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: households\n",
       "  Entities:\n",
       "    data [Rows: 33413, Columns: 146]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = ft.EntitySet(id = 'households')\n",
    "es.entity_from_dataframe(entity_id = 'data', \n",
    "                         dataframe = data, \n",
    "                         index = 'Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78b1b739b016732e5ad504f92f632eea3ebd92ed"
   },
   "source": [
    "# Normalize Household Table\n",
    "\n",
    "Normalization allows us to create another table with one unique row per instance. In this case, the instances are households. The new table is derived from the `data` table and we need to bring along any of the household level variables. Since these are the same for all members of a household, we can directly add these as columns in the household table using `additional_variables`. The index of the household table is `idhogar` which uniquely identifies each household.  \n",
    "\n",
    "All of the variable types have already been confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "ab897f0260e7dda326a0d0ecdbf1c613c1bf6088"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: households\n",
       "  Entities:\n",
       "    data [Rows: 33413, Columns: 42]\n",
       "    household [Rows: 10340, Columns: 105]\n",
       "  Relationships:\n",
       "    data.idhogar -> household.idhogar"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.normalize_entity(base_entity_id='data', \n",
    "                    new_entity_id='household', \n",
    "                    index = 'idhogar', \n",
    "                    additional_variables = hh_bool + hh_ordered + hh_cont + ['Target'])\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bb81ef01856ffe3d01bcf2ba6005dffda1c1836a"
   },
   "source": [
    "### Table Relationships\n",
    "\n",
    "Normalizing the entity automatically adds in the relationship between the parent, `household`, and the child, `ind`. This relationship links the two tables and allows us to create \"deep features\" by aggregating individuals in each household."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0808dac4226963c7d2c78d8a39bf3eae949d56f1"
   },
   "source": [
    "# Deep Feature Synthesis\n",
    "\n",
    "Here is where Featuretools gets to work. Using feature primitives, Deep Feature Synthesis can build hundreds (or 1000s as we will later see) of features from the relationships between tables and the columns in tables themselves. There are two types of primitives, which are operations applied to data:\n",
    "\n",
    "* Transforms: applied to one or more columns in a _single table_ of data \n",
    "* Aggregations: applied across _multiple tables_ using the relationships between tables\n",
    "\n",
    "We generate the features by calling `ft.dfs`. This build features using any of the applicable primitives for each column in the data. Featuretools uses the table relationships to aggregate features as required. For example, it will automatically aggregate the individual level data at the household level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c94754ac4ac46df6a9602763497e9048fee1378d"
   },
   "source": [
    "To start with, we use the default `agg` and `trans` primitives in a call to `ft.dfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "755c363b6648af063d4d5a2b4c944952dc91c64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 180 features\n",
      "EntitySet scattered to 8 workers in 16 seconds\n",
      "Elapsed: 00:34 | Progress: 100%|██████████\n"
     ]
    }
   ],
   "source": [
    "# Deep Feature Synthesis\n",
    "feature_matrix, feature_names = ft.dfs(entityset=es, \n",
    "                                       target_entity = 'household', \n",
    "                                       max_depth = 2, \n",
    "                                       verbose = 1, \n",
    "                                       n_jobs = -1, \n",
    "                                       chunk_size = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "bc4d85bf6106c212081a70597aaf5579c2a99c49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hacdor</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>paredzocalo</th>\n",
       "      <th>paredpreb</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>pareddes</th>\n",
       "      <th>paredmad</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENT_TRUE(data.instlevel9)</th>\n",
       "      <th>PERCENT_TRUE(data.instlevel5)</th>\n",
       "      <th>PERCENT_TRUE(data.estadocivil6)</th>\n",
       "      <th>PERCENT_TRUE(data.v18q)</th>\n",
       "      <th>PERCENT_TRUE(data.estadocivil7)</th>\n",
       "      <th>PERCENT_TRUE(data.estadocivil5)</th>\n",
       "      <th>PERCENT_TRUE(data.instlevel4)</th>\n",
       "      <th>PERCENT_TRUE(data.instlevel7)</th>\n",
       "      <th>PERCENT_TRUE(data.parentesco5)</th>\n",
       "      <th>PERCENT_TRUE(data.parentesco3)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idhogar</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21eb7fcc1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0e5d7a658</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2c7317ea8</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b58d945f</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d6dae86b7</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           hacdor  hacapo  v14a  refrig  paredblolad  paredzocalo  paredpreb  \\\n",
       "idhogar                                                                        \n",
       "21eb7fcc1   False   False  True    True         True        False      False   \n",
       "0e5d7a658   False   False  True    True        False        False      False   \n",
       "2c7317ea8   False   False  True    True        False        False      False   \n",
       "2b58d945f   False   False  True    True         True        False      False   \n",
       "d6dae86b7    True   False  True    True         True        False      False   \n",
       "\n",
       "           pisocemento  pareddes  paredmad  ...  \\\n",
       "idhogar                                     ...   \n",
       "21eb7fcc1        False     False     False  ...   \n",
       "0e5d7a658        False     False      True  ...   \n",
       "2c7317ea8        False     False      True  ...   \n",
       "2b58d945f        False     False     False  ...   \n",
       "d6dae86b7        False     False     False  ...   \n",
       "\n",
       "           PERCENT_TRUE(data.instlevel9)  PERCENT_TRUE(data.instlevel5)  \\\n",
       "idhogar                                                                   \n",
       "21eb7fcc1                            0.0                           0.00   \n",
       "0e5d7a658                            0.0                           0.00   \n",
       "2c7317ea8                            0.0                           1.00   \n",
       "2b58d945f                            0.0                           0.50   \n",
       "d6dae86b7                            0.0                           0.25   \n",
       "\n",
       "           PERCENT_TRUE(data.estadocivil6)  PERCENT_TRUE(data.v18q)  \\\n",
       "idhogar                                                               \n",
       "21eb7fcc1                              0.0                      0.0   \n",
       "0e5d7a658                              0.0                      1.0   \n",
       "2c7317ea8                              1.0                      0.0   \n",
       "2b58d945f                              0.0                      1.0   \n",
       "d6dae86b7                              0.0                      0.0   \n",
       "\n",
       "           PERCENT_TRUE(data.estadocivil7)  PERCENT_TRUE(data.estadocivil5)  \\\n",
       "idhogar                                                                       \n",
       "21eb7fcc1                             0.00                              0.0   \n",
       "0e5d7a658                             0.00                              0.0   \n",
       "2c7317ea8                             0.00                              0.0   \n",
       "2b58d945f                             0.25                              0.0   \n",
       "d6dae86b7                             0.25                              0.0   \n",
       "\n",
       "           PERCENT_TRUE(data.instlevel4)  PERCENT_TRUE(data.instlevel7)  \\\n",
       "idhogar                                                                   \n",
       "21eb7fcc1                           1.00                            0.0   \n",
       "0e5d7a658                           0.00                            0.0   \n",
       "2c7317ea8                           0.00                            0.0   \n",
       "2b58d945f                           0.25                            0.0   \n",
       "d6dae86b7                           0.25                            0.0   \n",
       "\n",
       "           PERCENT_TRUE(data.parentesco5)  PERCENT_TRUE(data.parentesco3)  \n",
       "idhogar                                                                    \n",
       "21eb7fcc1                             0.0                             0.0  \n",
       "0e5d7a658                             0.0                             0.0  \n",
       "2c7317ea8                             0.0                             0.0  \n",
       "2b58d945f                             0.0                             0.5  \n",
       "d6dae86b7                             0.0                             0.5  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = [str(x.get_name()) for x in feature_names]\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "9097cd1b974c7a5d0bae5fdaf48b2e30ce524cab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hacdor',\n",
       " 'hacapo',\n",
       " 'v14a',\n",
       " 'refrig',\n",
       " 'paredblolad',\n",
       " 'paredzocalo',\n",
       " 'paredpreb',\n",
       " 'pisocemento',\n",
       " 'pareddes',\n",
       " 'paredmad',\n",
       " 'paredzinc',\n",
       " 'paredfibras',\n",
       " 'paredother',\n",
       " 'pisomoscer',\n",
       " 'pisoother',\n",
       " 'pisonatur',\n",
       " 'pisonotiene',\n",
       " 'pisomadera',\n",
       " 'techozinc',\n",
       " 'techoentrepiso',\n",
       " 'techocane',\n",
       " 'techootro',\n",
       " 'cielorazo',\n",
       " 'abastaguadentro',\n",
       " 'abastaguafuera',\n",
       " 'abastaguano',\n",
       " 'planpri',\n",
       " 'noelec',\n",
       " 'coopele',\n",
       " 'sanitario1',\n",
       " 'sanitario2',\n",
       " 'sanitario3',\n",
       " 'sanitario5',\n",
       " 'sanitario6',\n",
       " 'energcocinar1',\n",
       " 'energcocinar2',\n",
       " 'energcocinar3',\n",
       " 'energcocinar4',\n",
       " 'elimbasu1',\n",
       " 'elimbasu2',\n",
       " 'elimbasu3',\n",
       " 'elimbasu4',\n",
       " 'elimbasu5',\n",
       " 'elimbasu6',\n",
       " 'epared1',\n",
       " 'epared2',\n",
       " 'epared3',\n",
       " 'etecho1',\n",
       " 'etecho2',\n",
       " 'etecho3',\n",
       " 'eviv1',\n",
       " 'eviv2',\n",
       " 'eviv3',\n",
       " 'tipovivi1',\n",
       " 'tipovivi2',\n",
       " 'tipovivi3',\n",
       " 'tipovivi4',\n",
       " 'tipovivi5',\n",
       " 'computer',\n",
       " 'television',\n",
       " 'lugar1',\n",
       " 'lugar2',\n",
       " 'lugar3',\n",
       " 'lugar4',\n",
       " 'lugar5',\n",
       " 'lugar6',\n",
       " 'area1',\n",
       " 'v2a1-missing',\n",
       " 'elec-missing',\n",
       " 'rooms',\n",
       " 'r4h1',\n",
       " 'r4h2',\n",
       " 'r4h3',\n",
       " 'r4m1',\n",
       " 'r4m2',\n",
       " 'r4m3',\n",
       " 'r4t1',\n",
       " 'r4t2',\n",
       " 'v18q1',\n",
       " 'tamviv',\n",
       " 'hhsize',\n",
       " 'hogar_nin',\n",
       " 'hhsize-diff',\n",
       " 'walls',\n",
       " 'roof',\n",
       " 'floor',\n",
       " 'walls+roof+floor',\n",
       " 'warning',\n",
       " 'bonus',\n",
       " 'hogar_adul',\n",
       " 'hogar_mayor',\n",
       " 'bedrooms',\n",
       " 'qmobilephone',\n",
       " 'v2a1',\n",
       " 'dependency',\n",
       " 'edjefe',\n",
       " 'edjefa',\n",
       " 'meaneduc',\n",
       " 'overcrowding',\n",
       " 'phones-per-capita',\n",
       " 'tablets-per-capita',\n",
       " 'rooms-per-capita',\n",
       " 'rent-per-capita',\n",
       " 'Target',\n",
       " 'SUM(data.rez_esc)',\n",
       " 'SUM(data.inst/age)',\n",
       " 'SUM(data.age)',\n",
       " 'SUM(data.inst)',\n",
       " 'SUM(data.escolari/age)',\n",
       " 'SUM(data.tech)',\n",
       " 'SUM(data.escolari)',\n",
       " 'STD(data.rez_esc)',\n",
       " 'STD(data.inst/age)',\n",
       " 'STD(data.age)',\n",
       " 'STD(data.inst)',\n",
       " 'STD(data.escolari/age)',\n",
       " 'STD(data.tech)',\n",
       " 'STD(data.escolari)',\n",
       " 'MAX(data.rez_esc)',\n",
       " 'MAX(data.inst/age)',\n",
       " 'MAX(data.age)',\n",
       " 'MAX(data.inst)',\n",
       " 'MAX(data.escolari/age)',\n",
       " 'MAX(data.tech)',\n",
       " 'MAX(data.escolari)',\n",
       " 'SKEW(data.rez_esc)',\n",
       " 'SKEW(data.inst/age)',\n",
       " 'SKEW(data.age)',\n",
       " 'SKEW(data.inst)',\n",
       " 'SKEW(data.escolari/age)',\n",
       " 'SKEW(data.tech)',\n",
       " 'SKEW(data.escolari)',\n",
       " 'MIN(data.rez_esc)',\n",
       " 'MIN(data.inst/age)',\n",
       " 'MIN(data.age)',\n",
       " 'MIN(data.inst)',\n",
       " 'MIN(data.escolari/age)',\n",
       " 'MIN(data.tech)',\n",
       " 'MIN(data.escolari)',\n",
       " 'MEAN(data.rez_esc)',\n",
       " 'MEAN(data.inst/age)',\n",
       " 'MEAN(data.age)',\n",
       " 'MEAN(data.inst)',\n",
       " 'MEAN(data.escolari/age)',\n",
       " 'MEAN(data.tech)',\n",
       " 'MEAN(data.escolari)',\n",
       " 'COUNT(data)',\n",
       " 'PERCENT_TRUE(data.parentesco12)',\n",
       " 'PERCENT_TRUE(data.parentesco10)',\n",
       " 'PERCENT_TRUE(data.dis)',\n",
       " 'PERCENT_TRUE(data.parentesco11)',\n",
       " 'PERCENT_TRUE(data.instlevel3)',\n",
       " 'PERCENT_TRUE(data.parentesco1)',\n",
       " 'PERCENT_TRUE(data.parentesco6)',\n",
       " 'PERCENT_TRUE(data.female)',\n",
       " 'PERCENT_TRUE(data.parentesco7)',\n",
       " 'PERCENT_TRUE(data.estadocivil1)',\n",
       " 'PERCENT_TRUE(data.instlevel8)',\n",
       " 'PERCENT_TRUE(data.instlevel1)',\n",
       " 'PERCENT_TRUE(data.estadocivil3)',\n",
       " 'PERCENT_TRUE(data.instlevel6)',\n",
       " 'PERCENT_TRUE(data.rez_esc-missing)',\n",
       " 'PERCENT_TRUE(data.estadocivil4)',\n",
       " 'PERCENT_TRUE(data.parentesco2)',\n",
       " 'PERCENT_TRUE(data.parentesco8)',\n",
       " 'PERCENT_TRUE(data.estadocivil2)',\n",
       " 'PERCENT_TRUE(data.instlevel2)',\n",
       " 'PERCENT_TRUE(data.mobilephone)',\n",
       " 'PERCENT_TRUE(data.parentesco4)',\n",
       " 'PERCENT_TRUE(data.parentesco9)',\n",
       " 'PERCENT_TRUE(data.instlevel9)',\n",
       " 'PERCENT_TRUE(data.instlevel5)',\n",
       " 'PERCENT_TRUE(data.estadocivil6)',\n",
       " 'PERCENT_TRUE(data.v18q)',\n",
       " 'PERCENT_TRUE(data.estadocivil7)',\n",
       " 'PERCENT_TRUE(data.estadocivil5)',\n",
       " 'PERCENT_TRUE(data.instlevel4)',\n",
       " 'PERCENT_TRUE(data.instlevel7)',\n",
       " 'PERCENT_TRUE(data.parentesco5)',\n",
       " 'PERCENT_TRUE(data.parentesco3)']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "80a52160943fcac264829ff40240df79659e04c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10340, 180)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "760ff2c661f5df3db42006ea4352535edefff631"
   },
   "source": [
    "# Feature Selection\n",
    "\n",
    "We can do some rudimentary feature selection, removing one of any pair of columns with a correlation greater than 0.99 (absolute value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "438501339f06e3a81979d52e1668850ff1eb1638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 columns with >= 0.99 correlation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MIN(data.tech)', 'MEAN(data.tech)', 'COUNT(data)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = feature_matrix.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 0.99)]\n",
    "\n",
    "print('There are {} columns with >= 0.99 correlation.'.format(len(to_drop)))\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "9b72d2c56f7e7f91e974c459af8865607c994ef4"
   },
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix[[x for x in feature_matrix if x not in to_drop]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d62229e5719042b87ff6787c06200f84ea682005"
   },
   "source": [
    "## Final Preparation of Training and Test Sets for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "e62563220cbe880894a208d28e649ed60c57d839",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = feature_matrix[feature_matrix['Target'].notnull()].reset_index()\n",
    "test = feature_matrix[feature_matrix['Target'].isnull()].reset_index()\n",
    "\n",
    "train = train[train['idhogar'].isin(list(train_valid['idhogar']))]\n",
    "test = test[test['idhogar'].isin(list(test_valid['idhogar']))]\n",
    "\n",
    "train_labels = np.array(train.pop('Target')).reshape((-1,))\n",
    "test_ids = list(test.pop('idhogar'))\n",
    "\n",
    "train, test = train.align(test, axis = 1, join = 'inner')\n",
    "all_features = list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hacdor</th>\n",
       "      <th>hacapo</th>\n",
       "      <th>v14a</th>\n",
       "      <th>refrig</th>\n",
       "      <th>paredblolad</th>\n",
       "      <th>paredzocalo</th>\n",
       "      <th>paredpreb</th>\n",
       "      <th>pisocemento</th>\n",
       "      <th>pareddes</th>\n",
       "      <th>paredmad</th>\n",
       "      <th>...</th>\n",
       "      <th>PERCENT_TRUE(data.instlevel9)</th>\n",
       "      <th>PERCENT_TRUE(data.instlevel5)</th>\n",
       "      <th>PERCENT_TRUE(data.estadocivil6)</th>\n",
       "      <th>PERCENT_TRUE(data.v18q)</th>\n",
       "      <th>PERCENT_TRUE(data.estadocivil7)</th>\n",
       "      <th>PERCENT_TRUE(data.estadocivil5)</th>\n",
       "      <th>PERCENT_TRUE(data.instlevel4)</th>\n",
       "      <th>PERCENT_TRUE(data.instlevel7)</th>\n",
       "      <th>PERCENT_TRUE(data.parentesco5)</th>\n",
       "      <th>PERCENT_TRUE(data.parentesco3)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hacdor  hacapo  v14a  refrig  paredblolad  paredzocalo  paredpreb  \\\n",
       "0   False   False  True    True         True        False      False   \n",
       "1   False   False  True    True        False        False      False   \n",
       "2   False   False  True    True        False        False      False   \n",
       "3   False   False  True    True         True        False      False   \n",
       "4    True   False  True    True         True        False      False   \n",
       "\n",
       "   pisocemento  pareddes  paredmad  ...  PERCENT_TRUE(data.instlevel9)  \\\n",
       "0        False     False     False  ...                            0.0   \n",
       "1        False     False      True  ...                            0.0   \n",
       "2        False     False      True  ...                            0.0   \n",
       "3        False     False     False  ...                            0.0   \n",
       "4        False     False     False  ...                            0.0   \n",
       "\n",
       "   PERCENT_TRUE(data.instlevel5)  PERCENT_TRUE(data.estadocivil6)  \\\n",
       "0                           0.00                              0.0   \n",
       "1                           0.00                              0.0   \n",
       "2                           1.00                              1.0   \n",
       "3                           0.50                              0.0   \n",
       "4                           0.25                              0.0   \n",
       "\n",
       "   PERCENT_TRUE(data.v18q)  PERCENT_TRUE(data.estadocivil7)  \\\n",
       "0                      0.0                             0.00   \n",
       "1                      1.0                             0.00   \n",
       "2                      0.0                             0.00   \n",
       "3                      1.0                             0.25   \n",
       "4                      0.0                             0.25   \n",
       "\n",
       "   PERCENT_TRUE(data.estadocivil5)  PERCENT_TRUE(data.instlevel4)  \\\n",
       "0                              0.0                           1.00   \n",
       "1                              0.0                           0.00   \n",
       "2                              0.0                           0.00   \n",
       "3                              0.0                           0.25   \n",
       "4                              0.0                           0.25   \n",
       "\n",
       "   PERCENT_TRUE(data.instlevel7)  PERCENT_TRUE(data.parentesco5)  \\\n",
       "0                            0.0                             0.0   \n",
       "1                            0.0                             0.0   \n",
       "2                            0.0                             0.0   \n",
       "3                            0.0                             0.0   \n",
       "4                            0.0                             0.0   \n",
       "\n",
       "   PERCENT_TRUE(data.parentesco3)  \n",
       "0                             0.0  \n",
       "1                             0.0  \n",
       "2                             0.0  \n",
       "3                             0.5  \n",
       "4                             0.5  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0cf45f8a476a9b781d7183d51c8d28bbbbd04951"
   },
   "source": [
    "## Modeling Exercise: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "d80f9c0acfeb4fa6f39ff2f70673e8bb28619fbb"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "592f86efe45657e56af6e8300be86b2e9430a251"
   },
   "outputs": [],
   "source": [
    "def macro_f1_score(labels, predictions):\n",
    "    # Reshape the predictions as needed\n",
    "    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n",
    "    \n",
    "    metric_value = f1_score(labels, predictions, average = 'macro')\n",
    "    \n",
    "    # Return is name, value, is_higher_better\n",
    "    return 'macro_f1', metric_value, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "b7ab95ac1b2a0b4ec6b23bf1693566946f19ae52"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "70aa9877b14ad7eeb19af201fb7748c656efd9ac"
   },
   "outputs": [],
   "source": [
    "def model_gbm(features, labels, test_features, test_ids, \n",
    "              nfolds = 5, return_preds = False, hyp = None):\n",
    "    \"\"\"Model using the GBM and cross validation.\n",
    "       Trains with early stopping on each fold.\n",
    "       Hyperparameters probably need to be tuned.\"\"\"\n",
    "    \n",
    "    feature_names = list(features.columns)\n",
    "    \n",
    "    hyp_OPTaaS = { 'boosting_type': 'dart',\n",
    "              'colsample_bytree': 0.9843467236959204,\n",
    "              'learning_rate': 0.11598629586769524,\n",
    "              'min_child_samples': 44,\n",
    "              'num_leaves': 49,\n",
    "              'reg_alpha': 0.35397370408131534,\n",
    "              'reg_lambda': 0.5904910774606467,\n",
    "              'subsample': 0.6299872254632797,\n",
    "              'subsample_for_bin': 60611}\n",
    "\n",
    "    # Model hyperparameters\n",
    "#     params = {'boosting_type': 'dart', \n",
    "#               'colsample_bytree': 0.88, \n",
    "#               'learning_rate': 0.028, \n",
    "#                'min_child_samples': 10, \n",
    "#                'num_leaves': 36, 'reg_alpha': 0.76, \n",
    "#                'reg_lambda': 0.43, \n",
    "#                'subsample_for_bin': 40000, \n",
    "#                'subsample': 0.54}\n",
    "\n",
    "    model = lgb.LGBMClassifier(**hyp_OPTaaS, class_weight = 'balanced',\n",
    "                               objective = 'multiclass', n_jobs = -1, n_estimators = 10000)\n",
    "    \n",
    "    # Using stratified kfold cross validation\n",
    "    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
    "    predictions = pd.DataFrame()\n",
    "    importances = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Convert to arrays for indexing\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    labels = np.array(labels).reshape((-1 ))\n",
    "    \n",
    "    valid_scores = []\n",
    "    \n",
    "    # Iterate through the folds\n",
    "    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n",
    "        # Dataframe for \n",
    "        fold_predictions = pd.DataFrame()\n",
    "        \n",
    "        # Training and validation data\n",
    "        X_train = features[train_indices]\n",
    "        X_valid = features[valid_indices]\n",
    "        y_train = labels[train_indices]\n",
    "        y_valid = labels[valid_indices]\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(X_train, y_train, early_stopping_rounds = 100, \n",
    "                  eval_metric = macro_f1_score,\n",
    "                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_names = ['train', 'valid'],\n",
    "                  verbose = 200)\n",
    "        \n",
    "        # Record the validation fold score\n",
    "        valid_scores.append(model.best_score_['valid']['macro_f1'])\n",
    "        \n",
    "        # Make predictions from the fold\n",
    "        fold_probabilitites = model.predict_proba(test_features)\n",
    "        \n",
    "        # Record each prediction for each class as a column\n",
    "        for j in range(4):\n",
    "            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n",
    "            \n",
    "        fold_predictions['idhogar'] = test_ids\n",
    "        fold_predictions['fold'] = (i+1)\n",
    "        predictions = predictions.append(fold_predictions)\n",
    "        \n",
    "        importances += model.feature_importances_ / nfolds   \n",
    "        \n",
    "        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n",
    "\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names,\n",
    "                                        'importance': importances})\n",
    "    valid_scores = np.array(valid_scores)\n",
    "    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n",
    "    \n",
    "    # If we want to examine predictions don't average over folds\n",
    "    if return_preds:\n",
    "        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "        return predictions, feature_importances\n",
    "    \n",
    "    # Average the predictions over folds\n",
    "    predictions = predictions.groupby('idhogar', as_index = False).mean()\n",
    "    \n",
    "    # Find the class and associated probability\n",
    "    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "    predictions = predictions.drop(columns = ['fold'])\n",
    "    \n",
    "    # Merge with the base to have one prediction for each individual\n",
    "    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n",
    "        \n",
    "    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n",
    "    \n",
    "    # return the submission and feature importances\n",
    "    return submission, feature_importances, valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "09b89dd670c8f2799aee6b6b7dbc16d0ca54981b",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fold 1, Validation Score: 0.3986, Estimators Trained: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fold 2, Validation Score: 0.36965, Estimators Trained: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fold 3, Validation Score: 0.39827, Estimators Trained: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fold 4, Validation Score: 0.42173, Estimators Trained: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Fold 5, Validation Score: 0.36843, Estimators Trained: 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5 cross validation score: 0.39134 with std: 0.0201.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "submission, feature_importances, valid_scores = model_gbm(train, train_labels, test, test_ids, 5)\n",
    "\n",
    "results = pd.DataFrame({'version': ['default_5fold'], \n",
    "                        'F1-mean': [valid_scores.mean()], \n",
    "                        'F1-std': [valid_scores.std()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "aa2fe688f07bc08a783c1f5ea78bc54122bcc634"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('ft_baseline.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Submission Score\n",
    "Based on the application of featuretools, we have got the following score (Macro F1 Score):\n",
    "- **Our notebook (Cross Validation)** - 0.40751\n",
    "- **Top Score (Public Leaderboard)** - 0.44878"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
